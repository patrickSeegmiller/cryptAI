{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Getting Started on a Cipher Diagnostic Tool</h1>\n",
    "\n",
    "<p>One of the most challenging aspects of cryptanalysis (at least with classical ciphers) is the identification of the type of cipher. Is it monoalphabetic, polygraphic, homophonic, or something else entirely?\n",
    "\n",
    "Once the correct type of cipher has been identified, the real work can start in earnest. This is easier said than done. \n",
    "\n",
    "Let's see if we can work toward a cipher classifier. This is going to take a lot of work, but with persistence and patience, I think we can produce something useful... but we need some data. In particular, we need to identify features of ciphers that can be used be the classifier in learning the structure of different ciphers.\n",
    "\n",
    "There are some clear choices. The ciphertext itself, and Friedman's Index of Coincidence (IoC) for a ciphertext are the most obvious starting places. Other important measures might include the Shannon entropy (though there may be some dependence between this and the IoC, so we will need to be careful), and the ngram frequency distributions. Polygraphic substitutions and homophonic substitutions can have a far fewer or greater number of characters that the standard English alphabet. It may be tempting to include known plaintext as well, but most cipher diagnosis  Let's start making a list.\n",
    "\n",
    "<ul>\n",
    "    <li>The Ciphertext Itself</li>\n",
    "    <li>Index of Coincidence</li>\n",
    "    <li>Shannon Entropy of the Ciphertext</li>\n",
    "    <li>N-Gram Frequency Distributions</li>\n",
    "    <li>Factors of Numbers Close to the Ciphertext Length</li>\n",
    "    <li>Other Things I Have Not Thought Of Yet!</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "This, admittedly, is not a large number of features to consider. We will keep our minds open.\n",
    "\n",
    "However, we are still getting ahead of ourselves! We don't even have any ciphers to work with! We also need to identify the kinds of assumptions that will underly our model, as that will impact the ways in which we construct our dataset.\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Assumptions</h2>\n",
    "Given a random ciphertext, how likely is it that is was encrypted with a Affine Cipher? Vigenere? Hill? \n",
    "\n",
    "Well, as I know of no \"Complete Database of Classical Ciphers,\" and since we want the classifier to base its conclusion on the characteristics of a given ciphertext, we will create this dataset in such a way that ciphers are approximately uniformly distributed. That is, a given ciphertext in the dataset will have an equal chance of having been enciphered with any of the encryption algorithms implemented.\n",
    "\n",
    "Furthermore, the likelihood of a ciphertext being encrypted with a given cipher should be independent of the length of the ciphertext. So we should be certain that ciphertext lengths be distributed among individual ciphers in approximately the same way as well. \n",
    "\n",
    "<ol>\n",
    "    <li>The distribution of ciphers used should be approximately uniform.</li>\n",
    "    <li>The distribution of lengths of a ciphertexts should be approximately uniform.</li>\n",
    "</ol>\n",
    "\n",
    "This looks like a good starting place, and we can return to these assumptions later if we feel so inclined. Let's make a dataset!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data</h2>\n",
    "\n",
    "For the time being, we are going to focus on ciphertext that has been encrypted from English plaintext. As such, Project Gutenburg seems like a good resource. Given their enormous influence in other texts, why don't we start with the top 100 downloaded ebooks of the last 30 days? \n",
    "\n",
    "It will probably be usefule to create regex object to identify the appropriate links on the webpage. A little poking around the source on https://www.gutenberg.org/browse/scores/top should help us identify the correct format.\n",
    "\n",
    "There appear to be a few ways the download urls appear. Here are the ones I have found so far (note I have formatted them according to\n",
    "the structure of f-strings):\n",
    "<ul>\n",
    "    <li>\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-0.txt\"</li>\n",
    "    <li>\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}.txt\"</li>\n",
    "    <li>\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}.txt.utf-8\"</li>\n",
    "    <li>\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-0.txt.utf-8\"</li>\n",
    "    <li>\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}.txt\"</li>\n",
    "    <li>\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}.txt.utf-8\"</li>\n",
    "    <li>\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}-0.txt\"</li>\n",
    "    <li>\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}-0.txt.utf-8</li>\n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay! Now let's create some functions to do the heavy lifting for us! \n",
    "\n",
    "First we will write a function to connect to the appropriate website, parse the html (using Beautiful Soup of course), get things organized by title, author, and ID number.\n",
    "\n",
    "After this, we will write a third function will construct urls for downloading each of the books with IDs retreived and a fourth function will attempt to download each of the books, trying a variety of file extensions and locations if the download fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that connects to the Project Gutenberg website for top books \n",
    "# and returns a dictionary of the most downloaded book titles and their IDs\n",
    "# where the keys are the book IDs and the values are the book titles and authors.\n",
    "\n",
    "def get_top_books_list():\n",
    "    \"\"\"\n",
    "    Connects to the Project Gutenberg website for top books and returns a dictionary of the most downloaded book titles and their IDs.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of tuples containing the book title, author, and ID number.\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Connect to the Project Gutenberg website for top books.\n",
    "    try:\n",
    "        response = requests.get(\"https://www.gutenberg.org/browse/scores/top\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    # Parse the HTML response.\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all possible book titles and ids and store them in a list\n",
    "    book_titles = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        try:\n",
    "            if link.get(\"href\").startswith(\"/ebooks/\"):\n",
    "                book_titles.append((link.text, link.get(\"href\").split(\"/\")[2]))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    # Remove the non-book titles at the front.\n",
    "    book_titles = book_titles[4:]\n",
    "\n",
    "    # Create a list with book titles and IDs.\n",
    "    book_list = []\n",
    "    for book in book_titles:\n",
    "        book_list.append((book[0], book[1]))\n",
    "\n",
    "    # Remove everything after and including 'by' from the book titles.\n",
    "    for i in range(len(book_list)):\n",
    "        book_list[i] = (book_list[i][0].split(\"by\")[0], book_list[i][1])\n",
    "\n",
    "    # Remove non-alpha characters from the book titles, remove excess whitespace, convert names to lowerspace,\n",
    "    # and replace remaining spaces with underscores.\n",
    "    for i in range(len(book_list)):\n",
    "        book_list[i] = (re.sub(r\"[^a-zA-Z ]+\", \"\", book_list[i][0]).strip().lower().replace(\" \", \"_\"), book_list[i][1])\n",
    "\n",
    "    # Remove books with duplicate ids.\n",
    "    for i in range(len(book_list)):\n",
    "        for j in range(len(book_list)):\n",
    "            try:\n",
    "                if i != j and book_list[i][1] == book_list[j][1]:\n",
    "                    book_list.pop(j)\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    return book_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('romeo_and_juliet', '1513'), ('mo', '2701'), ('a_room_with_a_view', '2641'), ('middlemarch', '145'), ('the_complete_works_of_william_shakespeare', '100'), ('little_women_or_meg_jo_beth_and_amy', '37106'), ('the_enchanted_april', '16389'), ('the_blue_castle_a_novel', '67979'), ('cranford', '394'), ('the_adventures_of_ferdinand_count_fathom__complete', '6761'), ('the_expedition_of_humphry_clinker', '2160'), ('the_adventures_of_roderick_random', '4085'), ('history_of_tom_jones_a_foundling', '6593'), ('twenty_years_after', '1259'), ('my_life__volume', '5197'), ('pride_and_prejudice', '1342'), ('frankenstein_or_the_modern_prometheus', '84'), ('alices_adventures_in_wonderland', '11'), ('dracula', '345'), ('the_great_gats', '64317'), ('the_picture_of_dorian_gray', '174'), ('the_photodrama', '70937'), ('a_tale_of_two_cities', '98'), ('the_wizards_cave', '70936'), ('noli_me_tangere', '20228'), ('a_modest_proposal', '1080'), ('ang_filibusterismo_karugtng_ng_noli_me_tangere', '47629'), ('the_adventures_of_sherlock_holmes', '1661'), ('the_yellow_wallpaper', '1952'), ('a_tale_of_three_weeks', '70935'), ('metamorphosis', '5200'), ('the_count_of_monte_cristo_illustrated', '1184'), ('the_importance_of_being_earnest_a_trivial_comedy_for_serious_people', '844'), ('ulysses', '4300'), ('the_brothers_karamazov', '28054'), ('crime_and_punishment', '2554'), ('the_scarlet_letter', '25344'), ('a_dolls_house__a_play', '2542'), ('great_expectations', '1400'), ('war_and_peace', '2600'), ('adventures_of_huckleberry_finn', '76'), ('grimms_fairy_tales', '2591'), ('the_kama_sutra_of_vatsyayana', '27827'), ('the_iliad', '6130'), ('jane_eyre_an_autobiography', '1260'), ('the_strange_case_of_dr_jekyll_and_mr_hyde', '43'), ('the_king_in_yellow', '8492'), ('thus_spake_zarathustra_a_book_for_all_and_none', '1998'), ('the_philippines_a_century_hence', '35899'), ('the_prince', '1232'), ('the_wonderful_wizard_of_oz', '55'), ('anna_karenina', '1399'), ('anne_of_green_gables', '45'), ('the_reign_of_greed', '10676'), ('florante_at_laura', '15845'), ('the_adventures_of_tom_sawyer_complete', '74'), ('the_romance_of_lust_a_classic_victorian_erotic_novel', '30254'), ('the_slang_dictionary', '42108'), ('tractatus_logicophilosophicus', '5740'), ('how_to_sing', '19116'), ('the_divine_comedy', '8800'), ('treasure_island', '120'), ('meditations', '2680'), ('the_war_of_the_worlds', '36'), ('the_idiot', '2638'), ('walden_and_on_the_duty_of_civil_disobedience', '205'), ('beyond_good_and_evil', '4363'), ('winniethepooh', '67098'), ('don_quixote', '996'), ('heart_of_darkness', '219'), ('autobiography_of_benjamin_franklin', '20203'), ('mo', '3206'), ('the_prophet', '58585'), ('wuthering_heights', '768'), ('a_study_in_scarlet', '244'), ('the_day_will_come', '70938'), ('the_odyssey', '1727'), ('little_women', '514'), ('knock_threeonetwo', '70944'), ('helps_to_latin_translation_at_sight', '28890'), ('the_king_james_version_of_the_bible', '10'), ('tish_plays_the_game', '70942'), ('the_jimmyjohns_and_other_stories', '70939'), ('les_misrables', '135'), ('peter_pan', '16'), ('bismarck_some_secret_pages_of_his_history_volume_ii_of', '70943'), ('the_republic', '1497'), ('constantinople_old_and_new', '70946'), ('a_christmas_carol_in_prose_being_a_ghost_story_of_christmas', '46'), ('dubliners', '2814'), ('calculus_made_easy', '33283'), ('the_souls_of_black_folk', '408'), ('three_men_in_a_boat_to_say_nothing_of_the_dog', '308'), ('carmilla', '10007'), ('emma', '158'), ('the_rmyan_of_vlmki_translated_into_english_verse', '24869'), ('baron_trumps_marvellous_underground_journey', '57426'), ('the_time_machine', '35'), ('david_copperfield', '766'), ('demonology_and_devillore', '40686'), ('medusas_coil', '70899'), ('diego_collados_grammar_of_the_japanese_language', '21197'), ('the_curse_of_yig', '70912'), ('the_jungle_book', '236'), ('memoirs_of_extraordinary_popular_delusions_and_the_madness_of_crowds', '24518'), ('the_problems_of_philosophy', '5827'), ('oliver_twist', '730'), ('sense_and_sensibility', '161'), ('the_hound_of_the_baskervilles', '2852'), ('notes_on_witchcraft', '70895'), ('the_murder_of_roger_ackroyd', '69087'), ('simple_sabotage_field_manual', '26184'), ('a_little_town_mouse', '70922'), ('a_dictionary_of_cebuano_visayan', '40074'), ('the_tempest', '23042'), ('siddhartha', '2500'), ('notes_from_the_underground', '600'), ('the_art_of_war', '132'), ('the_tragical_history_of_doctor_faustus', '779'), ('leviathan', '3207'), ('essays_of_michel_de_montaigne__complete', '3600')]\n"
     ]
    }
   ],
   "source": [
    "# Test the get_top_books_list() function.\n",
    "print(get_top_books_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a little bit fo work, but we got there in the end. \n",
    "\n",
    "Next, we make a function that takes an ID number as input and retrieves a plaintext file of the book with the given ID. Let's also have it name and save the file in a subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to download an ebook given its ID. Since Project Gutenberg is not a fan of automation, we will be sneaky\n",
    "# and wait for 1 second between requests.\n",
    "def download_ebook(ebook_id: str, save_path: str = \"text_files\") -> None:\n",
    "    \"\"\"\n",
    "    Downloads an ebook given its ID, and saves it to the text_files directory.\n",
    "    \n",
    "    Args:\n",
    "        ebook_id (str): The ID of the ebook to download.\n",
    "    \"\"\"\n",
    "\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    # Create a list of possible urls for the ebook.\n",
    "    urls = [f\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-0.txt\",\n",
    "            f\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}.txt\",\n",
    "            f\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-0.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}.txt\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}-0.txt\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}-0.txt.utf-8\"]\n",
    "    \n",
    "    # Try each url until we find one that works.\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # If the text response is an html file, rather than the book we are looking for, we try the next url.\n",
    "            if response.text.startswith(\"<!DOCTYPE html>\"):\n",
    "                response = None\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        # If the request fails, we try the next url.\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "            response = None\n",
    "            continue\n",
    "        finally:\n",
    "            # Wait 1 seconds between requests.\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Save the ebook to the text_files directory if the response is not None.\n",
    "    if response is not None:\n",
    "        with open(f\"{save_path}/{ebook_id}.txt\", \"w\") as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    # If it has not already been done, close the connection.\n",
    "    if response is not None:\n",
    "        response.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the download_ebook() function with something that should work (Treasure Island).\n",
    "download_ebook(\"112\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks promising! We will try to download them all, changing their names to something more appropriate along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to download all ebooks in the top 100 list, changing their names to their titles.\n",
    "def retrieve_books(file_path: str = \"text_files\") -> None:\n",
    "    \"\"\"\n",
    "    Downloads all ebooks in the top 100 list, changing their names to their titles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the os module.\n",
    "    import os\n",
    "\n",
    "    # Get the top 100 books list.\n",
    "    top_100_books = get_top_books_list()\n",
    "\n",
    "\n",
    "    # Download each ebook.\n",
    "    for book in top_100_books:\n",
    "        print(f\"Downloading {book[0]}...\")\n",
    "        download_ebook(book[1], file_path)\n",
    "\n",
    "        # Try to rename the ebook by the actual name. If the book already exists in the file path,\n",
    "        # move on to the next book. If an error occurs while trying to rename the book because\n",
    "        # the book could not be retreive in the first place, move on to the next book.\n",
    "        try:\n",
    "            os.rename(f\"{file_path}/{book[1]}.txt\", f\"{file_path}/{book[0]}.txt\")\n",
    "        except FileExistsError:\n",
    "            print(f\"{book[0]} already exists in this directory.\")\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Book with ID {book[1]} could not be found.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading romeo_and_juliet...\n",
      "Downloading mo...\n",
      "Downloading a_room_with_a_view...\n",
      "Downloading middlemarch...\n",
      "Downloading the_complete_works_of_william_shakespeare...\n",
      "Downloading little_women_or_meg_jo_beth_and_amy...\n",
      "Downloading the_enchanted_april...\n",
      "Downloading the_blue_castle_a_novel...\n",
      "Downloading cranford...\n",
      "Downloading the_adventures_of_ferdinand_count_fathom__complete...\n",
      "Downloading the_expedition_of_humphry_clinker...\n",
      "Downloading the_adventures_of_roderick_random...\n",
      "Downloading history_of_tom_jones_a_foundling...\n",
      "Downloading twenty_years_after...\n",
      "Downloading my_life__volume...\n",
      "Downloading pride_and_prejudice...\n",
      "Downloading frankenstein_or_the_modern_prometheus...\n",
      "Downloading alices_adventures_in_wonderland...\n",
      "Downloading dracula...\n",
      "Downloading the_great_gats...\n",
      "Downloading the_picture_of_dorian_gray...\n",
      "Downloading the_photodrama...\n",
      "Downloading a_tale_of_two_cities...\n",
      "Downloading the_wizards_cave...\n",
      "Downloading noli_me_tangere...\n",
      "Downloading a_modest_proposal...\n",
      "Downloading ang_filibusterismo_karugtng_ng_noli_me_tangere...\n",
      "Downloading the_adventures_of_sherlock_holmes...\n",
      "Downloading the_yellow_wallpaper...\n",
      "Downloading a_tale_of_three_weeks...\n",
      "Downloading metamorphosis...\n",
      "Downloading the_count_of_monte_cristo_illustrated...\n",
      "Downloading the_importance_of_being_earnest_a_trivial_comedy_for_serious_people...\n",
      "Downloading ulysses...\n",
      "Downloading the_brothers_karamazov...\n",
      "Downloading crime_and_punishment...\n",
      "Downloading the_scarlet_letter...\n",
      "Downloading a_dolls_house__a_play...\n",
      "Downloading great_expectations...\n",
      "Downloading war_and_peace...\n",
      "Downloading adventures_of_huckleberry_finn...\n",
      "Downloading grimms_fairy_tales...\n",
      "Downloading the_kama_sutra_of_vatsyayana...\n",
      "Downloading the_iliad...\n",
      "Downloading jane_eyre_an_autobiography...\n",
      "Downloading the_strange_case_of_dr_jekyll_and_mr_hyde...\n",
      "Downloading the_king_in_yellow...\n",
      "Downloading thus_spake_zarathustra_a_book_for_all_and_none...\n",
      "Downloading the_philippines_a_century_hence...\n",
      "Downloading the_prince...\n",
      "Downloading the_wonderful_wizard_of_oz...\n",
      "Downloading anna_karenina...\n",
      "Downloading anne_of_green_gables...\n",
      "Downloading the_reign_of_greed...\n",
      "Downloading florante_at_laura...\n",
      "Downloading the_adventures_of_tom_sawyer_complete...\n",
      "Downloading the_romance_of_lust_a_classic_victorian_erotic_novel...\n",
      "Downloading the_slang_dictionary...\n",
      "Downloading tractatus_logicophilosophicus...\n",
      "Book with ID 5740 could not be found.\n",
      "Downloading how_to_sing...\n",
      "Downloading the_divine_comedy...\n",
      "Downloading treasure_island...\n",
      "Downloading meditations...\n",
      "Downloading the_war_of_the_worlds...\n",
      "Downloading the_idiot...\n",
      "Downloading walden_and_on_the_duty_of_civil_disobedience...\n",
      "Downloading beyond_good_and_evil...\n",
      "Downloading winniethepooh...\n",
      "Downloading don_quixote...\n",
      "Downloading heart_of_darkness...\n",
      "Downloading autobiography_of_benjamin_franklin...\n",
      "Downloading mo...\n",
      "Downloading the_prophet...\n",
      "Downloading wuthering_heights...\n",
      "Downloading a_study_in_scarlet...\n",
      "Downloading the_day_will_come...\n",
      "Downloading the_odyssey...\n",
      "Downloading little_women...\n",
      "Downloading knock_threeonetwo...\n",
      "Downloading helps_to_latin_translation_at_sight...\n",
      "Downloading the_king_james_version_of_the_bible...\n",
      "Downloading tish_plays_the_game...\n",
      "Downloading the_jimmyjohns_and_other_stories...\n",
      "Downloading les_misrables...\n",
      "Downloading peter_pan...\n",
      "Downloading bismarck_some_secret_pages_of_his_history_volume_ii_of...\n",
      "Downloading the_republic...\n",
      "Downloading constantinople_old_and_new...\n",
      "Downloading a_christmas_carol_in_prose_being_a_ghost_story_of_christmas...\n",
      "Downloading dubliners...\n",
      "Downloading calculus_made_easy...\n",
      "Book with ID 33283 could not be found.\n",
      "Downloading the_souls_of_black_folk...\n",
      "Downloading three_men_in_a_boat_to_say_nothing_of_the_dog...\n",
      "Downloading carmilla...\n",
      "Downloading emma...\n",
      "Downloading the_rmyan_of_vlmki_translated_into_english_verse...\n",
      "Downloading baron_trumps_marvellous_underground_journey...\n",
      "Downloading the_time_machine...\n",
      "Downloading david_copperfield...\n",
      "Downloading demonology_and_devillore...\n",
      "Downloading medusas_coil...\n",
      "Downloading diego_collados_grammar_of_the_japanese_language...\n",
      "Downloading the_curse_of_yig...\n",
      "Downloading the_jungle_book...\n",
      "Downloading memoirs_of_extraordinary_popular_delusions_and_the_madness_of_crowds...\n",
      "Downloading the_problems_of_philosophy...\n",
      "Downloading oliver_twist...\n",
      "Downloading sense_and_sensibility...\n",
      "Downloading the_hound_of_the_baskervilles...\n",
      "Downloading notes_on_witchcraft...\n",
      "Downloading the_murder_of_roger_ackroyd...\n",
      "Downloading simple_sabotage_field_manual...\n",
      "Downloading a_little_town_mouse...\n",
      "Downloading a_dictionary_of_cebuano_visayan...\n",
      "Downloading the_tempest...\n",
      "Downloading siddhartha...\n",
      "Downloading notes_from_the_underground...\n",
      "Downloading the_art_of_war...\n",
      "Downloading the_tragical_history_of_doctor_faustus...\n",
      "Downloading leviathan...\n",
      "Downloading essays_of_michel_de_montaigne__complete...\n"
     ]
    }
   ],
   "source": [
    "# The moment of truth has arrived (again; I've come to this point a number of times). Download all the books.\n",
    "retrieve_books()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "# Check to see how many books were downloaded.\n",
    "print(len(os.listdir(\"text_files\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Success!... And More Issues to Address</h3>\n",
    "It looks as if things worked more or less according to plan (a few books are apparently unavailable in the file format I was hoping for). That said, we have some more things to deal with, like headers and footers to each text file, and there are some books which are not even in English (we can try an incorporate non-English text later on, but for now their inclusion will undermine some things).\n",
    "\n",
    "The style of writing (vocabularly, grammar, etc.) in these books is remarkably different and they come in a variety of lengths. For any cipher that is neither polyalphabetic nor polygraphic, the usage of different symbols, and therefore the vernacular of the period in which a text was written, will likely alter predictions due to differences in n-gram frequencies, particularly with $n > 1$. As such, we need to be sure that each book has an equal chance of being enciphered with a given cipher. In addition, since there is no reason someone cannot encipher sentence fragments, paraphrases, shorthand, or otherwise truncated text, we should try to ensure some of the plaintext possesses these characteristics.\n",
    "\n",
    "But first, let's clean things up a bit. We will create a function to remove the boilerplate text from the top and bottom of each ebook text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to remove the boilerplate text from a Project Gutenberg books.\n",
    "def remove_boilerplate(file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes the boilerplate text from a single Project Gutenberg books.\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    with open(f\"text_files/{file_name}\", \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Save the changes to the book.\n",
    "    with open(f\"text_files/{file_name}\", \"w\") as f:\n",
    "        f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, let's create a function to randomly select a piece of text (e.g., paragraphs, sentences, fragments) from one of these books, which we will also select randomly. Let's also try to ensure it doesn't cut words into pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to randomly select a book from the text_files directory, and then randomly select a bit of text from that book,\n",
    "# ensuring that the text does not cut words into pieces.\n",
    "import random\n",
    "def generate_random_text_samples(file_path: str, num_samples=1) -> list:\n",
    "    \"\"\"\n",
    "    Generates random text samples from the books in the text_files directory. The samples are randomly selected from the books. \n",
    "    The samples are guaranteed to be complete words, and the number of samples is determined by the num_samples parameter.   \n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the directory containing the text files.\n",
    "        num_samples (int): The number of samples to generate. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings containing the text samples.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If num_samples is not an integer greater than 0.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that num_samples is an integer greater than 0.\n",
    "    if num_samples < 1 or not isinstance(num_samples, int):\n",
    "        raise ValueError(\"num_samples must be an integer greater than 0.\")\n",
    "    \n",
    "    # Get a list of all the text files in the directory.\n",
    "    text_files = os.listdir(file_path)\n",
    "\n",
    "    # Select a random text file.\n",
    "    text_file = random.choice(text_files)\n",
    "\n",
    "    # Read the text file.\n",
    "    with open(file_path + \"/\" + text_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split the text into words.\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove punctuation from the words.\n",
    "    words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "\n",
    "    # Remove empty strings from the words.\n",
    "    words = [word for word in words if word != \"\"]\n",
    "\n",
    "    # Remove numbers from the words.\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "\n",
    "    # Loop until we have the desired number of samples.\n",
    "    samples = []\n",
    "    while len(samples) < num_samples:\n",
    "        # Select a random word.\n",
    "        word = random.choice(words)\n",
    "\n",
    "        # Get the index of the word.\n",
    "        index = words.index(word)\n",
    "\n",
    "        # Get the number of words in the text.\n",
    "        num_words = len(words)\n",
    "\n",
    "        # Get the number of words in the sample.\n",
    "        num_sample_words = random.randint(1, len(words)+1)\n",
    "\n",
    "        # Get the start and end indices of the sample.\n",
    "        start_index = index - num_sample_words // 2\n",
    "        end_index = index + num_sample_words // 2\n",
    "    \n",
    "        # If the sample would start before the beginning of the text, set the start index to 0.\n",
    "        if start_index < 0:\n",
    "            start_index = 0\n",
    "\n",
    "        # If the sample would end after the end of the text, set the end index to the end of the text.\n",
    "        if end_index > num_words:\n",
    "            end_index = num_words\n",
    "\n",
    "        # Get the sample.\n",
    "        sample = words[start_index:end_index]\n",
    "\n",
    "        # Join the words in the sample together.\n",
    "        sample = \" \".join(sample)\n",
    "\n",
    "        # Add the sample to the list of samples.\n",
    "        samples.append(sample)\n",
    "\n",
    "    # Return the samples.\n",
    "    return samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out by trying to produce 1000 text samples from our books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 random text samples using our new function.\n",
    "samples = generate_random_text_samples(\"text_files\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ciphertext_plaintext_pairs(cipher: Cipher, input_file: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads a text file, and then generates a file containing ciphertext and plaintext pairs of random length and randomly\n",
    "    selected starting positions. The ciphertext is constructed using a given cipher object.\n",
    "\n",
    "    Args:\n",
    "        cipher (Cipher): The cipher object to use for encryption.\n",
    "        input_file (str): The name of the file to read.\n",
    "        output_file (str): The name of the file to write.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the input file.\n",
    "    with open(input_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Generate ciphertext and plaintext pairs.\n",
    "    ciphertext_plaintext_pairs = []\n",
    "    for i in range(1000):\n",
    "        # Generate a random length for the ciphertext and plaintext.\n",
    "        length = random.randint(10, 1000)\n",
    "\n",
    "        # Generate a random starting position for the ciphertext and plaintext.\n",
    "        start = random.randint(0, len(text) - length)\n",
    "        \n",
    "        # Generate the ciphertext and plaintext. \n",
    "        plaintext = text[start:start+length]\n",
    "        ciphertext = cipher.encrypt(plaintext)\n",
    "\n",
    "        # Add the ciphertext and plaintext to the list of pairs.\n",
    "        ciphertext_plaintext_pairs.append((ciphertext, plaintext))\n",
    "\n",
    "    # Write the ciphertext and plaintext pairs to the output file.\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for pair in ciphertext_plaintext_pairs:\n",
    "            f.write(pair[0] + \"\\n\")\n",
    "            f.write(pair[1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonmachinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
