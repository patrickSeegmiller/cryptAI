{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Getting Started on a Cipher Diagnostic Tool</h1>\n",
    "\n",
    "<p>One of the most challenging aspects of cryptanalysis (at least with classical ciphers) is the identification of the type of cipher. Is it monoalphabetic, polygraphic, homophonic, or something else entirely?\n",
    "\n",
    "Once the correct type of cipher has been identified, the real work can start in earnest. This is easier said than done. \n",
    "\n",
    "Let's see if we can work toward a cipher classifier. This is going to take a lot of work, but with persistence and patience, I think we can produce something useful... but we need some data. In particular, we need to identify features of ciphers that can be used be the classifier in learning the structure of different ciphers.\n",
    "\n",
    "There are some clear choices. The ciphertext itself, and Friedman's Index of Coincidence (IoC) for a ciphertext are the most obvious starting places. Other important measures might include the Shannon entropy (though there may be some dependence between this and the IoC, so we will need to be careful), and the ngram frequency distributions. Polygraphic substitutions and homophonic substitutions can have a far fewer or greater number of characters that the standard English alphabet. It may be tempting to include known plaintext as well, but most cipher diagnosis  Let's start making a list.\n",
    "\n",
    "<ul>\n",
    "    <li>The Ciphertext Itself</li>\n",
    "    <li>Index of Coincidence</li>\n",
    "    <li>Shannon Entropy of the Ciphertext</li>\n",
    "    <li>N-Gram Frequency Distributions</li>\n",
    "    <li>Factors of Numbers Close to the Ciphertext Length</li>\n",
    "    <li>Other Things I Have Not Thought Of Yet!</li>\n",
    "\n",
    "</ul>\n",
    "\n",
    "This, admittedly, is not a large number of features to consider. We will keep our minds open.\n",
    "\n",
    "However, we are still getting ahead of ourselves! We don't even have any ciphers to work with! We also need to identify the kinds of assumptions that will underly our model, as that will impact the ways in which we construct our dataset.\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Assumptions</h2>\n",
    "Given a random ciphertext, how likely is it that is was encrypted with a Affine Cipher? Vigenere? Hill? \n",
    "\n",
    "Well, as I know of no \"Complete Database of Classical Ciphers,\" and since we want the classifier to base its conclusion on the characteristics of a given ciphertext, we will create this dataset in such a way that ciphers are approximately uniformly distributed. That is, a given ciphertext in the dataset will have an equal chance of having been enciphered with any of the encryption algorithms implemented.\n",
    "\n",
    "Furthermore, the likelihood of a ciphertext being encrypted with a given cipher should be independent of the length of the ciphertext. So we should be certain that ciphertext lengths be distributed among individual ciphers in approximately the same way as well. \n",
    "\n",
    "<ol>\n",
    "    <li>The distribution of ciphers used should be approximately uniform.</li>\n",
    "    <li>The distribution of lengths of a ciphertexts should be approximately uniform.</li>\n",
    "    <li>Encrypted text should come from human-readable plaintext.</li>\n",
    "    <li>Encrypted text should come from plaintext that (at least roughly) follows common grammatical conventions.</li>\n",
    "</ol>\n",
    "\n",
    "This looks like a good starting place, and we can return to these assumptions later if we feel so inclined. Let's make a dataset!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data</h2>\n",
    "\n",
    "<p>For the time being, we are going to focus on ciphertext that has been encrypted from English plaintext. As such, Project Gutenburg seems like a good resource. Given their enormous influence in other texts, why don't we start with the top 100 downloaded ebooks of the last 30 days? \n",
    "\n",
    "It will probably be usefule to create regex object to identify the appropriate links on the webpage. A little poking around the source on https://www.gutenberg.org/browse/scores/top should help us identify the correct format.\n",
    "\n",
    "There appear to be a few ways the download urls appear. Here are the ones I have found so far (note I have formatted them according to\n",
    "the structure of f-strings):\n",
    "<ul>\n",
    "    <li>\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-0.txt\"</li>\n",
    "    <li>\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}.txt\"</li>\n",
    "    <li>\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}.txt.utf-8\"</li>\n",
    "    <li>\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-0.txt.utf-8\"</li>\n",
    "    <li>\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}.txt\"</li>\n",
    "    <li>\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}.txt.utf-8\"</li>\n",
    "    <li>\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}-0.txt\"</li>\n",
    "    <li>\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}-0.txt.utf-8</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Okay! Now let's create some functions to do the heavy lifting for us! \n",
    "\n",
    "First we will write a function to connect to the appropriate website, parse the html (using Beautiful Soup of course), get things organized by title, author, and ID number.\n",
    "\n",
    "After this, we will write a third function will construct urls for downloading each of the books with IDs retreived and a fourth function will attempt to download each of the books, trying a variety of file extensions and locations if the download fails.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that connects to the Project Gutenberg website for top books \n",
    "# and returns a dictionary of the most downloaded book titles and their IDs\n",
    "# where the keys are the book IDs and the values are the book titles and authors.\n",
    "\n",
    "def get_top_books_list():\n",
    "    \"\"\"\n",
    "    Connects to the Project Gutenberg website for top books and returns a dictionary of the most downloaded book titles and their IDs.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of tuples containing the book title, author, and ID number.\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Connect to the Project Gutenberg website for top books.\n",
    "    try:\n",
    "        response = requests.get(\"https://www.gutenberg.org/browse/scores/top\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    # Parse the HTML response.\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all possible book titles and ids and store them in a list\n",
    "    book_titles = []\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        try:\n",
    "            if link.get(\"href\").startswith(\"/ebooks/\"):\n",
    "                book_titles.append((link.text, link.get(\"href\").split(\"/\")[2]))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    # Remove the non-book titles at the front.\n",
    "    book_titles = book_titles[4:]\n",
    "\n",
    "    # Create a list with book titles and IDs.\n",
    "    book_list = []\n",
    "    for book in book_titles:\n",
    "        book_list.append((book[0], book[1]))\n",
    "\n",
    "    # Remove everything after and including 'by' from the book titles.\n",
    "    for i in range(len(book_list)):\n",
    "        book_list[i] = (book_list[i][0].split(\"by\")[0], book_list[i][1])\n",
    "\n",
    "    # Remove non-alpha characters from the book titles, remove excess whitespace, convert names to lowerspace,\n",
    "    # and replace remaining spaces with underscores.\n",
    "    for i in range(len(book_list)):\n",
    "        book_list[i] = (re.sub(r\"[^a-zA-Z ]+\", \"\", book_list[i][0]).strip().lower().replace(\" \", \"_\"), book_list[i][1])\n",
    "\n",
    "    # Remove books with duplicate ids.\n",
    "    for i in range(len(book_list)):\n",
    "        for j in range(len(book_list)):\n",
    "            try:\n",
    "                if i != j and book_list[i][1] == book_list[j][1]:\n",
    "                    book_list.pop(j)\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    return book_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('romeo_and_juliet', '1513'), ('mo', '2701'), ('a_room_with_a_view', '2641'), ('middlemarch', '145'), ('little_women_or_meg_jo_beth_and_amy', '37106'), ('the_complete_works_of_william_shakespeare', '100'), ('the_enchanted_april', '16389'), ('the_blue_castle_a_novel', '67979'), ('cranford', '394'), ('the_adventures_of_ferdinand_count_fathom__complete', '6761'), ('history_of_tom_jones_a_foundling', '6593'), ('the_expedition_of_humphry_clinker', '2160'), ('the_adventures_of_roderick_random', '4085'), ('twenty_years_after', '1259'), ('my_life__volume', '5197'), ('pride_and_prejudice', '1342'), ('frankenstein_or_the_modern_prometheus', '84'), ('the_devils_dictionary', '972'), ('alices_adventures_in_wonderland', '11'), ('constantinople_old_and_new', '70946'), ('the_yellow_wallpaper', '1952'), ('knock_threeonetwo', '70944'), ('the_great_gats', '64317'), ('a_tale_of_two_cities', '98'), ('the_picture_of_dorian_gray', '174'), ('dracula', '345'), ('adventures_of_huckleberry_finn', '76'), ('noli_me_tangere', '20228'), ('the_adventures_of_sherlock_holmes', '1661'), ('the_brothers_karamazov', '28054'), ('metamorphosis', '5200'), ('the_count_of_monte_cristo_illustrated', '1184'), ('the_adventures_of_tom_sawyer_complete', '74'), ('the_importance_of_being_earnest_a_trivial_comedy_for_serious_people', '844'), ('a_modest_proposal', '1080'), ('crime_and_punishment', '2554'), ('grimms_fairy_tales', '2591'), ('ulysses', '4300'), ('the_prince', '1232'), ('the_strange_case_of_dr_jekyll_and_mr_hyde', '43'), ('in_memoriam', '70950'), ('war_and_peace', '2600'), ('ang_filibusterismo_karugtng_ng_noli_me_tangere', '47629'), ('the_kama_sutra_of_vatsyayana', '27827'), ('great_expectations', '1400'), ('a_christmas_carol_in_prose_being_a_ghost_story_of_christmas', '46'), ('the_king_in_yellow', '8492'), ('the_romance_of_lust_a_classic_victorian_erotic_novel', '30254'), ('a_dolls_house__a_play', '2542'), ('the_iliad', '6130'), ('mo', '3206'), ('the_scarlet_letter', '25344'), ('jane_eyre_an_autobiography', '1260'), ('don_quixote', '996'), ('meditations', '2680'), ('the_wonderful_wizard_of_oz', '55'), ('oliver_twist', '730'), ('anna_karenina', '1399'), ('thus_spake_zarathustra_a_book_for_all_and_none', '1998'), ('bismarck', '70943'), ('a_walk_in_the_grisons', '70947'), ('anne_of_green_gables', '45'), ('schwnke', '70945'), ('treasure_island', '120'), ('beyond_good_and_evil', '4363'), ('wuthering_heights', '768'), ('tractatus_logicophilosophicus', '5740'), ('a_study_in_scarlet', '244'), ('the_prophet', '58585'), ('baron_trumps_marvellous_underground_journey', '57426'), ('the_rmyan_of_vlmki_translated_into_english_verse', '24869'), ('the_day_will_come', '70938'), ('the_divine_comedy', '8800'), ('peter_pan', '16'), ('walden_and_on_the_duty_of_civil_disobedience', '205'), ('the_slang_dictionary', '42108'), ('helps_to_latin_translation_at_sight', '28890'), ('les_misrables', '135'), ('winniethepooh', '67098'), ('tish_plays_the_game', '70942'), ('the_war_of_the_worlds', '36'), ('the_philippines_a_century_hence', '35899'), ('carmilla', '10007'), ('little_women', '514'), ('heart_of_darkness', '219'), ('the_reign_of_greed', '10676'), ('the_republic', '1497'), ('the_odyssey', '1727'), ('three_men_in_a_boat_to_say_nothing_of_the_dog', '308'), ('demonology_and_devillore', '40686'), ('how_to_sing', '19116'), ('the_king_james_version_of_the_bible', '10'), ('through_the_lookingglass', '12'), ('the_time_machine', '35'), ('emma', '158'), ('dubliners', '2814'), ('calculus_made_easy', '33283'), ('david_copperfield', '766'), ('the_jungle_book', '236'), ('the_jimmyjohns_and_other_stories', '70939'), ('florante_at_laura', '15845'), ('the_curse_of_yig', '70912'), ('memoirs_of_extraordinary_popular_delusions_and_the_madness_of_crowds', '24518'), ('diego_collados_grammar_of_the_japanese_language', '21197'), ('the_problems_of_philosophy', '5827'), ('the_souls_of_black_folk', '408'), ('a_little_town_mouse', '70922'), ('sense_and_sensibility', '161'), ('the_murder_of_roger_ackroyd', '69087'), ('the_hound_of_the_baskervilles', '2852'), ('simple_sabotage_field_manual', '26184'), ('siddhartha', '2500'), ('the_tempest', '23042'), ('notes_from_the_underground', '600'), ('the_art_of_war', '132'), ('the_tragical_history_of_doctor_faustus', '779'), ('essays_of_michel_de_montaigne__complete', '3600'), ('leviathan', '3207')]\n"
     ]
    }
   ],
   "source": [
    "# Test the get_top_books_list() function.\n",
    "print(get_top_books_list())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It took a little bit fo work, but we got there in the end. \n",
    "\n",
    "Next, we make a function that takes an ID number as input and retrieves a plaintext file of the book with the given ID. Let's also have it name and save the file in a subdirectory.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to download an ebook given its ID. Since Project Gutenberg is not a fan of automated tools messing with their\n",
    "# site, we will be way sneaky and wait for 1 second between requests.\n",
    "def download_ebook(ebook_id: str, save_path: str = \"text_files\") -> None:\n",
    "    \"\"\"\n",
    "    Downloads an ebook given its ID, and saves it to the text_files directory.\n",
    "    \n",
    "    Args:\n",
    "        ebook_id (str): The ID of the ebook to download.\n",
    "    \"\"\"\n",
    "\n",
    "    import requests\n",
    "    import time\n",
    "\n",
    "    # Create a list of possible urls for the ebook.\n",
    "    urls = [f\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-0.txt\",\n",
    "            f\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}.txt\",\n",
    "            f\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/files/{ebook_id}/{ebook_id}-0.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}.txt\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}.txt.utf-8\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}-0.txt\",\n",
    "            f\"https://www.gutenberg.org/cache/epub/{ebook_id}/pg{ebook_id}-0.txt.utf-8\"]\n",
    "    \n",
    "    # Try each url until we find one that works.\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # If the text response is an html file, rather than the book we are looking for, we try the next url.\n",
    "            if response.text.startswith(\"<!DOCTYPE html>\"):\n",
    "                response = None\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        # If the request fails, we try the next url.\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "            response = None\n",
    "            continue\n",
    "        finally:\n",
    "            # Wait 1 seconds between requests.\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Save the ebook to the text_files directory if the response is not None.\n",
    "    if response is not None:\n",
    "        with open(f\"{save_path}/{ebook_id}.txt\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    # If it has not already been done, close the connection.\n",
    "    if response is not None:\n",
    "        response.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the download_ebook() function with something that should work (At The Earth's Core by Edgar Rice Burroughs).\n",
    "download_ebook(\"123\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks promising! We will try to download them all, changing their names to something more appropriate along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to download all ebooks in the top 100 list, changing their names to their titles.\n",
    "def retrieve_books(file_path: str = \"text_files\") -> None:\n",
    "    \"\"\"\n",
    "    Downloads all ebooks in the top 100 list, changing their names to their titles.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the os module.\n",
    "    import os\n",
    "\n",
    "    # Get the top 100 books list.\n",
    "    top_100_books = get_top_books_list()\n",
    "\n",
    "\n",
    "    # Download each ebook.\n",
    "    for book in top_100_books:\n",
    "        print(f\"Downloading {book[0]}...\")\n",
    "        download_ebook(book[1], file_path)\n",
    "\n",
    "        # Try to rename the ebook by the actual name. If the book already exists in the file path,\n",
    "        # move on to the next book. If an error occurs while trying to rename the book because\n",
    "        # the book could not be retreive in the first place, move on to the next book.\n",
    "        try:\n",
    "            os.rename(f\"{file_path}/{book[1]}.txt\", f\"{file_path}/{book[0]}.txt\")\n",
    "        except FileExistsError:\n",
    "            print(f\"{book[0]} already exists in this directory.\")\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Book with ID {book[1]} could not be found.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading romeo_and_juliet...\n",
      "Downloading mo...\n",
      "Downloading a_room_with_a_view...\n",
      "Downloading middlemarch...\n",
      "Downloading little_women_or_meg_jo_beth_and_amy...\n",
      "Downloading the_complete_works_of_william_shakespeare...\n",
      "Downloading the_enchanted_april...\n",
      "Downloading the_blue_castle_a_novel...\n",
      "Downloading cranford...\n",
      "Downloading the_adventures_of_ferdinand_count_fathom__complete...\n",
      "Downloading history_of_tom_jones_a_foundling...\n",
      "Downloading the_expedition_of_humphry_clinker...\n",
      "Downloading the_adventures_of_roderick_random...\n",
      "Downloading twenty_years_after...\n",
      "Downloading my_life__volume...\n",
      "Downloading pride_and_prejudice...\n",
      "Downloading frankenstein_or_the_modern_prometheus...\n",
      "Downloading the_devils_dictionary...\n",
      "Downloading alices_adventures_in_wonderland...\n",
      "Downloading constantinople_old_and_new...\n",
      "Downloading the_yellow_wallpaper...\n",
      "Downloading knock_threeonetwo...\n",
      "Downloading the_great_gats...\n",
      "Downloading a_tale_of_two_cities...\n",
      "Downloading the_picture_of_dorian_gray...\n",
      "Downloading dracula...\n",
      "Downloading adventures_of_huckleberry_finn...\n",
      "Downloading noli_me_tangere...\n",
      "Downloading the_adventures_of_sherlock_holmes...\n",
      "Downloading the_brothers_karamazov...\n",
      "Downloading metamorphosis...\n",
      "Downloading the_count_of_monte_cristo_illustrated...\n",
      "Downloading the_adventures_of_tom_sawyer_complete...\n",
      "Downloading the_importance_of_being_earnest_a_trivial_comedy_for_serious_people...\n",
      "Downloading a_modest_proposal...\n",
      "Downloading crime_and_punishment...\n",
      "Downloading grimms_fairy_tales...\n",
      "Downloading ulysses...\n",
      "Downloading the_prince...\n",
      "Downloading the_strange_case_of_dr_jekyll_and_mr_hyde...\n",
      "Downloading in_memoriam...\n",
      "Downloading war_and_peace...\n",
      "Downloading ang_filibusterismo_karugtng_ng_noli_me_tangere...\n",
      "Downloading the_kama_sutra_of_vatsyayana...\n",
      "Downloading great_expectations...\n",
      "Downloading a_christmas_carol_in_prose_being_a_ghost_story_of_christmas...\n",
      "Downloading the_king_in_yellow...\n",
      "Downloading the_romance_of_lust_a_classic_victorian_erotic_novel...\n",
      "Downloading a_dolls_house__a_play...\n",
      "Downloading the_iliad...\n",
      "Downloading mo...\n",
      "Downloading the_scarlet_letter...\n",
      "Downloading jane_eyre_an_autobiography...\n",
      "Downloading don_quixote...\n",
      "Downloading meditations...\n",
      "Downloading the_wonderful_wizard_of_oz...\n",
      "Downloading oliver_twist...\n",
      "Downloading anna_karenina...\n",
      "Downloading thus_spake_zarathustra_a_book_for_all_and_none...\n",
      "Downloading bismarck...\n",
      "Downloading a_walk_in_the_grisons...\n",
      "Downloading anne_of_green_gables...\n",
      "Downloading schwnke...\n",
      "Downloading treasure_island...\n",
      "Downloading beyond_good_and_evil...\n",
      "Downloading wuthering_heights...\n",
      "Downloading tractatus_logicophilosophicus...\n",
      "Book with ID 5740 could not be found.\n",
      "Downloading a_study_in_scarlet...\n",
      "Downloading the_prophet...\n",
      "Downloading baron_trumps_marvellous_underground_journey...\n",
      "Downloading the_rmyan_of_vlmki_translated_into_english_verse...\n",
      "Downloading the_day_will_come...\n",
      "Downloading the_divine_comedy...\n",
      "Downloading peter_pan...\n",
      "Downloading walden_and_on_the_duty_of_civil_disobedience...\n",
      "Downloading the_slang_dictionary...\n",
      "Downloading helps_to_latin_translation_at_sight...\n",
      "Downloading les_misrables...\n",
      "Downloading winniethepooh...\n",
      "Downloading tish_plays_the_game...\n",
      "Downloading the_war_of_the_worlds...\n",
      "Downloading the_philippines_a_century_hence...\n",
      "Downloading carmilla...\n",
      "Downloading little_women...\n",
      "Downloading heart_of_darkness...\n",
      "Downloading the_reign_of_greed...\n",
      "Downloading the_republic...\n",
      "Downloading the_odyssey...\n",
      "Downloading three_men_in_a_boat_to_say_nothing_of_the_dog...\n",
      "Downloading demonology_and_devillore...\n",
      "Downloading how_to_sing...\n",
      "Downloading the_king_james_version_of_the_bible...\n",
      "Downloading through_the_lookingglass...\n",
      "Downloading the_time_machine...\n",
      "Downloading emma...\n",
      "Downloading dubliners...\n",
      "Downloading calculus_made_easy...\n",
      "Book with ID 33283 could not be found.\n",
      "Downloading david_copperfield...\n",
      "Downloading the_jungle_book...\n",
      "Downloading the_jimmyjohns_and_other_stories...\n",
      "Downloading florante_at_laura...\n",
      "Downloading the_curse_of_yig...\n",
      "Downloading memoirs_of_extraordinary_popular_delusions_and_the_madness_of_crowds...\n",
      "Downloading diego_collados_grammar_of_the_japanese_language...\n",
      "Downloading the_problems_of_philosophy...\n",
      "Downloading the_souls_of_black_folk...\n",
      "Downloading a_little_town_mouse...\n",
      "Downloading sense_and_sensibility...\n",
      "Downloading the_murder_of_roger_ackroyd...\n",
      "Downloading the_hound_of_the_baskervilles...\n",
      "Downloading simple_sabotage_field_manual...\n",
      "Downloading siddhartha...\n",
      "Downloading the_tempest...\n",
      "Downloading notes_from_the_underground...\n",
      "Downloading the_art_of_war...\n",
      "Downloading the_tragical_history_of_doctor_faustus...\n",
      "Downloading essays_of_michel_de_montaigne__complete...\n",
      "Downloading leviathan...\n"
     ]
    }
   ],
   "source": [
    "# The moment of truth has arrived (again; I've come to this point a number of times). Download all the books.\n",
    "retrieve_books()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    }
   ],
   "source": [
    "# Check to see how many books were downloaded.\n",
    "print(len(os.listdir(\"text_files\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Success!... And More Issues to Address</h3>\n",
    "\n",
    "<p>It looks as if things worked more or less according to plan (a few books are apparently unavailable in the file format I was hoping for). That said, we have some more things to deal with, like headers and footers to each text file, and there are some books which are not even in English (we can try to incorporate non-English text later on, but for now their inclusion will undermine some things).\n",
    "\n",
    "The style of writing (vocabularly, grammar, etc.) in these books is remarkably different and they come in a variety of lengths. For any cipher that is neither polyalphabetic nor polygraphic, the usage of different symbols, and therefore the vernacular of the period in which a text was written, will likely alter predictions due to differences in n-gram frequencies, particularly with $n > 1$. As such, we need to be sure that each book has an equal chance of being enciphered with a given cipher. In addition, since there is no reason someone cannot encipher sentence fragments, paraphrases, shorthand, or otherwise truncated text, we should try to ensure some of the plaintext possesses these characteristics.\n",
    "\n",
    "But first, let's clean things up a bit. We will create a function to remove the boilerplate text from the top and bottom of each ebook text file, and to clean up some of the spacing and formatting issues.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to clean up the text from a Project Gutenberg book.\n",
    "def clean_text(file_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes the boilerplate text from a single Project Gutenberg book and cleans the text up by\n",
    "    fixing punctuation and removing non-ascii characters.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the file to clean.\n",
    "\n",
    "    Raises:\n",
    "        UnicodeDecodeError: If the file cannot be decoded.\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    # Open the file and read the text.\n",
    "    try:\n",
    "        with open(f\"text_files/{file_name}\", \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError: {file_name} could not be processed.\")\n",
    "        return None\n",
    "\n",
    "    # Remove the boilerplate text.\n",
    "    text = text.split(\"START OF THE PROJECT GUTENBERG EBOOK\")[1]\n",
    "    text = text.split(\"END OF THE PROJECT GUTENBERG EBOOK\")[0]\n",
    "    \n",
    "    # Remove the first and last lines of the text to get rid of the redundant title and asterisks.\n",
    "    text = \" \".join(text.split(\"\\n\")[1:-1])\n",
    "\n",
    "    # Text cleaning. The only punctuation we will retain are commas and any symbols used to terminate a sentence.\n",
    "    # Remove the newlines.\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "\n",
    "    # Remove the chapter headings.\n",
    "    text = re.sub(\"CHAPTER [A-Z]+\", \"\", text)\n",
    "\n",
    "    # Remove the asterisks, quotation marks, apostrophes, dashes, semicolons, colons, parentheses, brackets, and underscores.\n",
    "    text = re.sub(\"[\\*\\\"\\'\\-\\;\\:\\(\\)\\[\\]\\_]\", \"\", text)\n",
    "\n",
    "    # Remove any remaining non-ascii characters.\n",
    "    text = re.sub(\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "    # Remove any spaces that are more than one character long.\n",
    "    text = re.sub(\" {2,}\", \" \", text)\n",
    "\n",
    "    # Save the text back to the file.\n",
    "    try:\n",
    "        with open(f\"text_files/{file_name}\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "    except UnicodeEncodeError:\n",
    "        print(f\"UnicodeEncodeError: {file_name} could not be processed.\")\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to clean all the texts in the text_files directory.\n",
    "def clean_all_texts(file_path: str = \"text_files\") -> None:\n",
    "    \"\"\"\n",
    "    Cleans all the texts in the text_files directory.\n",
    "\n",
    "    Args:\n",
    "        file_path (str, optional): The path to the directory containing the text files. Defaults to \"text_files\". Any\n",
    "        of the text files that cannot be cleaned will be deleted.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file path does not exist.\n",
    "        IndexError: If the text cannot be cleaned, it will be deleted.\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "\n",
    "    # Get a list of all the books in the text_files directory.\n",
    "    books = os.listdir(file_path)\n",
    "\n",
    "    # Remove the boilerplate text from each book.\n",
    "    for book in books:\n",
    "        try:\n",
    "            clean_text(book)\n",
    "        except IndexError:\n",
    "            print(f\"{book} could not be processed.\")\n",
    "            # If the text cannot be cleaned, delete the file.\n",
    "            os.remove(f\"{file_path}/{book}\")\n",
    "            continue\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{book} could not be found.\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_philippines_a_century_hence.txt could not be processed.\n",
      "ang_filibusterismo_karugtng_ng_noli_me_tangere.txt could not be processed.\n",
      "the_tragical_history_of_doctor_faustus.txt could not be processed.\n",
      "UnicodeDecodeError: .DS_Store could not be processed.\n",
      "helps_to_latin_translation_at_sight.txt could not be processed.\n",
      "demonology_and_devillore.txt could not be processed.\n",
      "the_kama_sutra_of_vatsyayana.txt could not be processed.\n",
      "noli_me_tangere.txt could not be processed.\n",
      "florante_at_laura.txt could not be processed.\n",
      "the_expedition_of_humphry_clinker.txt could not be processed.\n",
      "the_problems_of_philosophy.txt could not be processed.\n",
      "how_to_sing.txt could not be processed.\n",
      "the_king_in_yellow.txt could not be processed.\n",
      "mo.txt could not be processed.\n",
      "the_slang_dictionary.txt could not be processed.\n",
      "baron_trumps_marvellous_underground_journey.txt could not be processed.\n",
      "diego_collados_grammar_of_the_japanese_language.txt could not be processed.\n",
      "the_tempest.txt could not be processed.\n",
      "history_of_tom_jones_a_foundling.txt could not be processed.\n",
      "beyond_good_and_evil.txt could not be processed.\n",
      "the_reign_of_greed.txt could not be processed.\n"
     ]
    }
   ],
   "source": [
    "# Clean all the texts in the text_files directory.\n",
    "clean_all_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "# Check to see how many books remain. \n",
    "print(len(os.listdir(\"text_files\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Whew! That Was a Learning Experience!</h3>\n",
    "\n",
    "<p>Now that we have some files to work with, we need to get back to the issue of ensuring each book is equally likely to be enciphered with a given cipher. To that end, we will create a function that will randomly select a book from among the books we have downloaded, and then it will randomly select a substring of the book. The substring will be of a random length, and it will be selected from a random location in the book. We will also have it return the index of the substring within the book.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to randomly select a book from the text_files directory and select a random excerpt from that book.\n",
    "def get_random_excerpt(file_path: str = \"text_files\") -> str:\n",
    "    \"\"\"\n",
    "    Randomly selects a book from the text_files directory and selects a random excerpt from that book.\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    import random\n",
    "\n",
    "    # Get a list of all the books in the text_files directory.\n",
    "    books = os.listdir(file_path)\n",
    "\n",
    "    # Remove the .DS_Store file if it exists.\n",
    "    if \".DS_Store\" in books:\n",
    "        books.remove(\".DS_Store\")\n",
    "\n",
    "    # Select a random book\n",
    "    book = random.choice(books) \n",
    "\n",
    "    # Open the book and read the text.\n",
    "    try:\n",
    "        with open(f\"{file_path}/{book}\", \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"UnicodeDecodeError: {book} could not be processed.\")\n",
    "        return None\n",
    "\n",
    "    # Randomly select a length for the excerpt between 1 and 100000 characters.\n",
    "    excerpt_length = random.randint(1, 100000)\n",
    "\n",
    "    # Randomly select a starting point for the excerpt.\n",
    "    excerpt_start = random.randint(0, len(text) - excerpt_length) if len(text) > excerpt_length else 0\n",
    "\n",
    "    # Select the excerpt.\n",
    "    excerpt = text[excerpt_start:excerpt_start + excerpt_length] if len(text) > excerpt_length else text\n",
    "\n",
    "    return excerpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Test the get_random_excerpt() function a few thousand times.\n",
    "for i in range(10000):\n",
    "    get_random_excerpt()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>So far so good!</h3>\n",
    "\n",
    "<p>A little testing with the get_random_excerpt() function has revealed that at least one issue remains. Some of the books remaining are not in English. We will have to deal with this later. For now, let's just remove them from the list of books we are working with.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English texts from the text_files directory.\n",
    "def remove_non_english_texts(file_path: str = \"text_files\") -> None:\n",
    "    \"\"\"\n",
    "    Removes non-English texts from the text_files directory. This function uses the langdetect library to detect the\n",
    "    language of the text. If the language is not English, the text is deleted.\n",
    "\n",
    "    Args:\n",
    "        file_path (str, optional): The path to the directory containing the text files. Defaults to \"text_files\".\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Import the os library.\n",
    "    import os\n",
    "\n",
    "    # Get a list of all the books in the text_files directory.\n",
    "    books = os.listdir(file_path)\n",
    "\n",
    "    # Remove the .DS_Store file if it exists.\n",
    "    if \".DS_Store\" in books:\n",
    "        books.remove(\".DS_Store\")\n",
    "\n",
    "    # Remove non-English texts.\n",
    "    for book in books:\n",
    "        try:\n",
    "            if detect(get_random_excerpt()) != \"en\":\n",
    "                os.remove(f\"{file_path}/{book}\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"UnicodeDecodeError: {book} could not be removed.\")\n",
    "            continue\n",
    "        except TypeError:\n",
    "            print(f\"TypeError: {book} could not be removed.\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ciphertext_plaintext_pairs(cipher: Cipher, input_file: str, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Reads a text file, and then generates a file containing ciphertext and plaintext pairs of random length and randomly\n",
    "    selected starting positions. The ciphertext is constructed using a given cipher object.\n",
    "\n",
    "    Args:\n",
    "        cipher (Cipher): The cipher object to use for encryption.\n",
    "        input_file (str): The name of the file to read.\n",
    "        output_file (str): The name of the file to write.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the input file.\n",
    "    with open(input_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Generate ciphertext and plaintext pairs.\n",
    "    ciphertext_plaintext_pairs = []\n",
    "    for i in range(1000):\n",
    "        # Generate a random length for the ciphertext and plaintext.\n",
    "        length = random.randint(10, 1000)\n",
    "\n",
    "        # Generate a random starting position for the ciphertext and plaintext.\n",
    "        start = random.randint(0, len(text) - length)\n",
    "        \n",
    "        # Generate the ciphertext and plaintext. \n",
    "        plaintext = text[start:start+length]\n",
    "        ciphertext = cipher.encrypt(plaintext)\n",
    "\n",
    "        # Add the ciphertext and plaintext to the list of pairs.\n",
    "        ciphertext_plaintext_pairs.append((ciphertext, plaintext))\n",
    "\n",
    "    # Write the ciphertext and plaintext pairs to the output file.\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for pair in ciphertext_plaintext_pairs:\n",
    "            f.write(pair[0] + \"\\n\")\n",
    "            f.write(pair[1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonmachinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
